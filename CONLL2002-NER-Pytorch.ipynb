{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_tags_from_sents(sents):\n",
    "    tokens, tags = [], []\n",
    "    for sent in sents:\n",
    "        sent_tokens, _, sent_tags = list(zip(*sent))\n",
    "        tokens.append(sent_tokens)\n",
    "        tags.append(sent_tags)\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.9 ms, sys: 6.83 ms, total: 70.7 ms\n",
      "Wall time: 70.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_tokens, train_tags = get_tokens_tags_from_sents(train_sents)\n",
    "val_tokens, val_tags = get_tokens_tags_from_sents(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>(</td>\n",
       "      <td>Australia</td>\n",
       "      <td>)</td>\n",
       "      <td>,</td>\n",
       "      <td>25</td>\n",
       "      <td>may</td>\n",
       "      <td>(</td>\n",
       "      <td>EFE</td>\n",
       "      <td>)</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1          2  3  4   5    6  7      8  9  10\n",
       "0  Melbourne  (  Australia  )  ,  25  may  (    EFE  )  .\n",
       "1      B-LOC  O      B-LOC  O  O   O    O  O  B-ORG  O  O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "pd.DataFrame([train_tokens[idx], train_tags[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dictionaries\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Now you need to implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    ind = 0\n",
    "    for t in special_tokens:\n",
    "        tok2idx[t] = ind\n",
    "        ind += 1\n",
    "    for sam in tokens_or_tags:\n",
    "        for t in sam:\n",
    "            if t not in special_tokens and t not in tok2idx:\n",
    "                tok2idx[t] = ind\n",
    "                ind += 1\n",
    "    return tok2idx, dict((v,k) for k,v in tok2idx.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens for tokens will be:\n",
    " - `<UNK>` token for out of vocabulary tokens; index = 0\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of sentences. index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<PAD>` token. It is also a good practice to provide RNN with sequence lengths, so it can skip computations for padding parts. We provide the batching function *batches_generator* readily available for you to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    return torch.tensor(np.vectorize(lambda t: to_ix.get(t, to_ix.get(\"<UNK>\", to_ix['O'])))(seq), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True, seed=8):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    # TODO: use DataLoader from Pytorch for this\n",
    "    # tokens is a list of docs, and each docs is a list of tokens\n",
    "    # SHUFFLE\n",
    "    n_samples = len(tokens)\n",
    "    np.random.seed(seed)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    # NUMBER OF BATCHES\n",
    "    n_batches = n_samples // batch_size\n",
    "    # and n_samples / batch_size not integer, put the leftovers in last batch\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    # for each batch, get the docs, labels and real lengths and yield them\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        x, y = [], []\n",
    "        batch_lengths = torch.zeros(batch_end - batch_start, dtype=torch.int32)\n",
    "        # x will be a list of lists of indices (one list of indices per doc in this batch)\n",
    "        for sample_in_batch_index, sample_idx in enumerate(order[batch_start: batch_end]):\n",
    "            try:\n",
    "                x.append(prepare_sequence(tokens[sample_idx], token2idx))\n",
    "                y.append(prepare_sequence(tags[sample_idx], tag2idx))\n",
    "            except Exception as marc:\n",
    "                print(marc)\n",
    "                import pdb; pdb.set_trace()\n",
    "            batch_lengths[sample_in_batch_index] = len(tags[sample_idx])\n",
    "        x = pad_sequence(x, batch_first=True, padding_value=token2idx[\"<PAD>\"])\n",
    "        y = pad_sequence(y, batch_first=True, padding_value=-1)\n",
    "        batch_lengths, perm_idx = batch_lengths.sort(0, descending=True)\n",
    "        # yield each batch\n",
    "        yield x[perm_idx, ...], y[perm_idx, ...], batch_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import (\n",
    "    pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
    ")\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "torch.manual_seed(1)\n",
    "import numpy as np\n",
    "from sklearn_crfsuite.metrics import flat_classification_report, flat_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, embedding_dim, hidden_dim, vocab_size, tagset_size,\n",
    "                padding_idx, verbose=False, bidirectional=False):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,\n",
    "                           bidirectional=bidirectional)\n",
    "        self.tagset_size = tagset_size\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear((1+bidirectional)*hidden_dim, tagset_size)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, X, X_lens):\n",
    "        # embeddings\n",
    "        embeds = self.word_embeddings(X)\n",
    "        if self.verbose: print(f\"Embeds: {embeds.size()}\")\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        embeds = pack_padded_sequence(embeds, X_lens.cpu().numpy(), batch_first=True)\n",
    "        # lstm\n",
    "        #lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # undo the packing operation\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        if self.verbose: print(f\"lstm_out: {lstm_out.size()}\")\n",
    "        # (batch_size, seq_len, hidden_dim) --> (batch_size * seq_len, hidden_dim)\n",
    "        s = lstm_out.contiguous().view(-1, lstm_out.shape[2])\n",
    "        # (batch_size * seq_len, hidden_dim) --> (batch_size * seq_len, tag_dim)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        if self.verbose: print(f\"tag space: {tag_space.size()}\")\n",
    "        # normalize logits\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        if self.verbose: print(f\"tag scores: {tag_scores.size()}\")\n",
    "        return tag_scores\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        return criterion(Y_hat.view(-1, Y_hat.size()[2]), Y.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-ORG', 'B-PER', 'I-PER', 'B-MISC', 'I-ORG', 'I-LOC', 'I-MISC']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_score = list(tag2idx.keys())\n",
    "labels_to_score.remove('O')\n",
    "labels_to_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels_to_score, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, batch_tokens, batch_lengths, batch_tags=None):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    tag_scores = model(batch_tokens, batch_lengths)\n",
    "    predicted_tags = np.vectorize(idx2tag.get)(torch.argmax(tag_scores, dim=2).data.numpy())\n",
    "    if batch_tags is not None:\n",
    "        return predicted_tags, model.loss(tag_scores, batch_tags)\n",
    "    return predicted_tags\n",
    "\n",
    "\n",
    "def my_scorer(true_tags, predicted_tags):\n",
    "    logger.info(flat_f1_score(true_tags, predicted_tags, average='weighted', labels=sorted_labels))\n",
    "    logger.info(flat_classification_report(\n",
    "        true_tags, predicted_tags, labels=sorted_labels, digits=3\n",
    "    ))\n",
    "\n",
    "\n",
    "def eval_model_for_set(model, tokens, tags, scoring=my_scorer):\n",
    "    \"\"\"Computes NER quality measures given model and a dataset\"\"\"\n",
    "    model.eval()\n",
    "    predicted_tags, true_tags, loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, lengths in batches_generator(1, tokens, tags):\n",
    "            padded_predicted_tags, batch_loss = predict_tags(model, x_batch, lengths, y_batch)\n",
    "            loss += batch_loss\n",
    "            padded_true_tags = np.vectorize(idx2tag.get)(y_batch.data)\n",
    "            for x, y, l in zip(padded_predicted_tags, padded_true_tags, lengths): \n",
    "                predicted_tags.append(x[:l])\n",
    "                true_tags.append(y[:l])    \n",
    "        scoring(true_tags, predicted_tags)\n",
    "        return loss / len(true_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(training_data, i, model, word2idx, idx2tag):\n",
    "    # Note that element i,j of tag_scores is the score for tag j for word i.\n",
    "    # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        seq = training_data[0][i]\n",
    "        labs = training_data[1][i]\n",
    "        inputs = prepare_sequence(seq, word2idx)\n",
    "        tag_scores = model(inputs.view(1, len(inputs)), \n",
    "                           torch.tensor([len(seq)]))\n",
    "        tags = np.vectorize(idx2tag.get)(torch.argmax(tag_scores, dim=2).data.numpy())\n",
    "        print(seq)\n",
    "        print()\n",
    "        print(tags)\n",
    "        print()\n",
    "        print(len(seq), tag_scores.size(), tags.shape)\n",
    "        print()\n",
    "        print(training_data[1][i])\n",
    "        print(training_data[1][i] == tags)       \n",
    "#print_example(training_data, 79, model, token2idx, idx2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparams and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 200\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "VOCAB_SIZE = len(token2idx)\n",
    "TAGSET_SIZE = len(tag2idx)\n",
    "PADDING_IDX = token2idx[\"<PAD>\"]\n",
    "training_data = (train_tokens, train_tags)\n",
    "model = LSTMTagger(BATCH_SIZE, EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, \n",
    "                   TAGSET_SIZE, PADDING_IDX, verbose=False, bidirectional=True)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-08 13:56:41,937 - START!\n",
      "2019-03-08 13:59:20,601 - avg epoch 1 train loss: 0.3225\n",
      "2019-03-08 14:01:47,314 - avg epoch 2 train loss: 0.1231\n",
      "2019-03-08 14:04:11,705 - avg epoch 3 train loss: 0.0644\n",
      "2019-03-08 14:06:37,306 - avg epoch 4 train loss: 0.0391\n",
      "2019-03-08 14:09:09,900 - avg epoch 5 train loss: 0.0275\n",
      "2019-03-08 14:09:09,901 - **********TRAINING PERFORMANCE*********\n",
      "2019-03-08 14:10:15,848 - 0.6647981460439756\n",
      "2019-03-08 14:10:17,747 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.640     0.700     0.669      4913\n",
      "       I-LOC      0.415     0.682     0.516      1891\n",
      "      B-MISC      0.461     0.771     0.577      2173\n",
      "      I-MISC      0.382     0.660     0.484      3212\n",
      "       B-ORG      0.752     0.720     0.736      7390\n",
      "       I-ORG      0.690     0.591     0.637      4992\n",
      "       B-PER      0.610     0.831     0.704      4321\n",
      "       I-PER      0.725     0.863     0.788      3903\n",
      "\n",
      "   micro avg      0.601     0.724     0.657     32795\n",
      "   macro avg      0.584     0.727     0.639     32795\n",
      "weighted avg      0.629     0.724     0.665     32795\n",
      "\n",
      "2019-03-08 14:10:17,767 - **********VALIDATION PERFORMANCE*********\n",
      "2019-03-08 14:10:29,958 - 0.5166884437939634\n",
      "2019-03-08 14:10:30,333 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.545     0.506     0.525      1084\n",
      "       I-LOC      0.251     0.489     0.332       325\n",
      "      B-MISC      0.258     0.528     0.346       339\n",
      "      I-MISC      0.237     0.436     0.307       557\n",
      "       B-ORG      0.658     0.601     0.629      1400\n",
      "       I-ORG      0.569     0.406     0.474      1104\n",
      "       B-PER      0.469     0.709     0.564       735\n",
      "       I-PER      0.569     0.746     0.645       634\n",
      "\n",
      "   micro avg      0.463     0.552     0.504      6178\n",
      "   macro avg      0.444     0.553     0.478      6178\n",
      "weighted avg      0.509     0.552     0.517      6178\n",
      "\n",
      "2019-03-08 14:10:30,339 - Loss: 0.5622300505638123\n",
      "2019-03-08 14:23:16,211 - avg epoch 6 train loss: 0.0191\n",
      "2019-03-08 14:25:55,673 - avg epoch 7 train loss: 0.0157\n",
      "2019-03-08 14:28:37,392 - avg epoch 8 train loss: 0.0144\n",
      "2019-03-08 14:31:09,755 - avg epoch 9 train loss: 0.0125\n",
      "2019-03-08 14:33:43,889 - avg epoch 10 train loss: 0.0156\n",
      "2019-03-08 14:33:43,890 - **********TRAINING PERFORMANCE*********\n",
      "2019-03-08 14:34:47,721 - 0.6903879900808061\n",
      "2019-03-08 14:34:49,645 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.617     0.693     0.653      4913\n",
      "       I-LOC      0.442     0.751     0.556      1891\n",
      "      B-MISC      0.450     0.818     0.581      2173\n",
      "      I-MISC      0.422     0.709     0.529      3212\n",
      "       B-ORG      0.783     0.747     0.764      7390\n",
      "       I-ORG      0.692     0.739     0.715      4992\n",
      "       B-PER      0.641     0.829     0.723      4321\n",
      "       I-PER      0.705     0.897     0.789      3903\n",
      "\n",
      "   micro avg      0.614     0.767     0.682     32795\n",
      "   macro avg      0.594     0.773     0.664     32795\n",
      "weighted avg      0.639     0.767     0.690     32795\n",
      "\n",
      "2019-03-08 14:34:49,663 - **********VALIDATION PERFORMANCE*********\n",
      "2019-03-08 14:35:01,853 - 0.531483383116481\n",
      "2019-03-08 14:35:02,219 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.518     0.506     0.512      1084\n",
      "       I-LOC      0.249     0.511     0.334       325\n",
      "      B-MISC      0.222     0.525     0.312       339\n",
      "      I-MISC      0.240     0.481     0.320       557\n",
      "       B-ORG      0.671     0.601     0.635      1400\n",
      "       I-ORG      0.595     0.552     0.573      1104\n",
      "       B-PER      0.484     0.694     0.570       735\n",
      "       I-PER      0.525     0.773     0.625       634\n",
      "\n",
      "   micro avg      0.457     0.584     0.513      6178\n",
      "   macro avg      0.438     0.580     0.485      6178\n",
      "weighted avg      0.508     0.584     0.531      6178\n",
      "\n",
      "2019-03-08 14:35:02,224 - Loss: 0.5988790988922119\n",
      "2019-03-08 14:37:33,985 - avg epoch 11 train loss: 0.0111\n",
      "2019-03-08 14:40:07,290 - avg epoch 12 train loss: 0.0101\n",
      "2019-03-08 14:42:40,926 - avg epoch 13 train loss: 0.0092\n",
      "2019-03-08 14:45:12,348 - avg epoch 14 train loss: 0.0083\n",
      "2019-03-08 14:47:59,384 - avg epoch 15 train loss: 0.0082\n",
      "2019-03-08 14:47:59,385 - **********TRAINING PERFORMANCE*********\n",
      "2019-03-08 14:49:03,528 - 0.7250087771696683\n",
      "2019-03-08 14:49:05,478 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.676     0.752     0.712      4913\n",
      "       I-LOC      0.462     0.810     0.588      1891\n",
      "      B-MISC      0.495     0.854     0.627      2173\n",
      "      I-MISC      0.437     0.767     0.557      3212\n",
      "       B-ORG      0.817     0.758     0.786      7390\n",
      "       I-ORG      0.752     0.772     0.762      4992\n",
      "       B-PER      0.663     0.845     0.743      4321\n",
      "       I-PER      0.760     0.884     0.817      3903\n",
      "\n",
      "   micro avg      0.649     0.796     0.715     32795\n",
      "   macro avg      0.633     0.805     0.699     32795\n",
      "weighted avg      0.680     0.796     0.725     32795\n",
      "\n",
      "2019-03-08 14:49:05,497 - **********VALIDATION PERFORMANCE*********\n",
      "2019-03-08 14:49:17,609 - 0.5478277128097938\n",
      "2019-03-08 14:49:17,963 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.522     0.558     0.540      1084\n",
      "       I-LOC      0.255     0.609     0.360       325\n",
      "      B-MISC      0.238     0.504     0.323       339\n",
      "      I-MISC      0.253     0.503     0.336       557\n",
      "       B-ORG      0.697     0.622     0.657      1400\n",
      "       I-ORG      0.626     0.552     0.586      1104\n",
      "       B-PER      0.445     0.712     0.547       735\n",
      "       I-PER      0.568     0.774     0.656       634\n",
      "\n",
      "   micro avg      0.467     0.607     0.528      6178\n",
      "   macro avg      0.450     0.604     0.501      6178\n",
      "weighted avg      0.522     0.607     0.548      6178\n",
      "\n",
      "2019-03-08 14:49:17,968 - Loss: 0.6217414140701294\n",
      "2019-03-08 14:51:50,242 - avg epoch 16 train loss: 0.0080\n",
      "2019-03-08 14:54:21,078 - avg epoch 17 train loss: 0.0092\n",
      "2019-03-08 14:56:53,349 - avg epoch 18 train loss: 0.0081\n",
      "2019-03-08 14:59:25,773 - avg epoch 19 train loss: 0.0078\n",
      "2019-03-08 15:01:57,404 - avg epoch 20 train loss: 0.0066\n",
      "2019-03-08 15:01:57,405 - **********TRAINING PERFORMANCE*********\n",
      "2019-03-08 15:02:59,540 - 0.7421182414490232\n",
      "2019-03-08 15:03:01,463 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.729     0.738     0.733      4913\n",
      "       I-LOC      0.464     0.767     0.578      1891\n",
      "      B-MISC      0.504     0.861     0.636      2173\n",
      "      I-MISC      0.462     0.767     0.576      3212\n",
      "       B-ORG      0.845     0.791     0.817      7390\n",
      "       I-ORG      0.750     0.803     0.776      4992\n",
      "       B-PER      0.682     0.853     0.758      4321\n",
      "       I-PER      0.766     0.896     0.826      3903\n",
      "\n",
      "   micro avg      0.671     0.807     0.733     32795\n",
      "   macro avg      0.650     0.810     0.713     32795\n",
      "weighted avg      0.700     0.807     0.742     32795\n",
      "\n",
      "2019-03-08 15:03:01,483 - **********VALIDATION PERFORMANCE*********\n",
      "2019-03-08 15:03:13,927 - 0.5648142251154116\n",
      "2019-03-08 15:03:14,268 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.591     0.532     0.560      1084\n",
      "       I-LOC      0.275     0.526     0.361       325\n",
      "      B-MISC      0.245     0.504     0.330       339\n",
      "      I-MISC      0.259     0.478     0.336       557\n",
      "       B-ORG      0.719     0.620     0.666      1400\n",
      "       I-ORG      0.628     0.582     0.604      1104\n",
      "       B-PER      0.532     0.664     0.590       735\n",
      "       I-PER      0.604     0.784     0.682       634\n",
      "\n",
      "   micro avg      0.505     0.596     0.546      6178\n",
      "   macro avg      0.482     0.586     0.516      6178\n",
      "weighted avg      0.555     0.596     0.565      6178\n",
      "\n",
      "2019-03-08 15:03:14,273 - Loss: 0.6311703324317932\n",
      "2019-03-08 15:05:46,601 - avg epoch 21 train loss: 0.0066\n",
      "2019-03-08 15:08:18,346 - avg epoch 22 train loss: 0.0090\n",
      "2019-03-08 15:10:50,047 - avg epoch 23 train loss: 0.0101\n",
      "2019-03-08 15:13:24,176 - avg epoch 24 train loss: 0.0097\n",
      "2019-03-08 15:16:12,707 - avg epoch 25 train loss: 0.0086\n",
      "2019-03-08 15:16:12,708 - **********TRAINING PERFORMANCE*********\n",
      "2019-03-08 15:17:18,596 - 0.7278626027813864\n",
      "2019-03-08 15:17:20,535 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.703     0.743     0.722      4913\n",
      "       I-LOC      0.454     0.819     0.584      1891\n",
      "      B-MISC      0.497     0.848     0.627      2173\n",
      "      I-MISC      0.443     0.754     0.558      3212\n",
      "       B-ORG      0.815     0.777     0.796      7390\n",
      "       I-ORG      0.755     0.745     0.750      4992\n",
      "       B-PER      0.704     0.844     0.768      4321\n",
      "       I-PER      0.715     0.908     0.800      3903\n",
      "\n",
      "   micro avg      0.655     0.796     0.719     32795\n",
      "   macro avg      0.636     0.805     0.701     32795\n",
      "weighted avg      0.684     0.796     0.728     32795\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-08 15:17:20,553 - **********VALIDATION PERFORMANCE*********\n",
      "2019-03-08 15:17:32,846 - 0.5427281004220501\n",
      "2019-03-08 15:17:33,198 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.554     0.551     0.553      1084\n",
      "       I-LOC      0.230     0.557     0.326       325\n",
      "      B-MISC      0.251     0.528     0.341       339\n",
      "      I-MISC      0.240     0.463     0.316       557\n",
      "       B-ORG      0.691     0.611     0.648      1400\n",
      "       I-ORG      0.627     0.530     0.574      1104\n",
      "       B-PER      0.495     0.653     0.563       735\n",
      "       I-PER      0.544     0.754     0.632       634\n",
      "\n",
      "   micro avg      0.471     0.585     0.522      6178\n",
      "   macro avg      0.454     0.581     0.494      6178\n",
      "weighted avg      0.528     0.585     0.543      6178\n",
      "\n",
      "2019-03-08 15:17:33,203 - Loss: 0.6535874605178833\n",
      "2019-03-08 15:20:07,192 - avg epoch 26 train loss: 0.0077\n",
      "2019-03-08 15:22:41,176 - avg epoch 27 train loss: 0.0067\n",
      "2019-03-08 15:25:17,347 - avg epoch 28 train loss: 0.0065\n",
      "2019-03-08 15:27:50,804 - avg epoch 29 train loss: 0.0066\n",
      "2019-03-08 15:30:25,347 - avg epoch 30 train loss: 0.0058\n",
      "2019-03-08 15:30:25,348 - **********TRAINING PERFORMANCE*********\n",
      "2019-03-08 15:31:30,475 - 0.7454617969025537\n",
      "2019-03-08 15:31:32,391 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.720     0.765     0.742      4913\n",
      "       I-LOC      0.494     0.814     0.615      1891\n",
      "      B-MISC      0.519     0.834     0.640      2173\n",
      "      I-MISC      0.477     0.801     0.598      3212\n",
      "       B-ORG      0.823     0.776     0.799      7390\n",
      "       I-ORG      0.777     0.804     0.790      4992\n",
      "       B-PER      0.708     0.835     0.766      4321\n",
      "       I-PER      0.733     0.911     0.812      3903\n",
      "\n",
      "   micro avg      0.677     0.811     0.738     32795\n",
      "   macro avg      0.656     0.817     0.720     32795\n",
      "weighted avg      0.702     0.811     0.745     32795\n",
      "\n",
      "2019-03-08 15:31:32,410 - **********VALIDATION PERFORMANCE*********\n",
      "2019-03-08 15:31:44,376 - 0.5500537269569958\n",
      "2019-03-08 15:31:44,728 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.597     0.539     0.566      1084\n",
      "       I-LOC      0.271     0.557     0.365       325\n",
      "      B-MISC      0.242     0.513     0.329       339\n",
      "      I-MISC      0.279     0.467     0.349       557\n",
      "       B-ORG      0.695     0.595     0.641      1400\n",
      "       I-ORG      0.625     0.565     0.594      1104\n",
      "       B-PER      0.497     0.660     0.567       735\n",
      "       I-PER      0.507     0.781     0.615       634\n",
      "\n",
      "   micro avg      0.488     0.589     0.534      6178\n",
      "   macro avg      0.464     0.585     0.503      6178\n",
      "weighted avg      0.538     0.589     0.550      6178\n",
      "\n",
      "2019-03-08 15:31:44,733 - Loss: 0.6658124327659607\n",
      "2019-03-08 15:34:19,580 - avg epoch 31 train loss: 0.0053\n",
      "2019-03-08 15:36:53,391 - avg epoch 32 train loss: 0.0059\n",
      "2019-03-08 15:39:28,715 - avg epoch 33 train loss: 0.0054\n",
      "2019-03-08 15:42:05,058 - avg epoch 34 train loss: 0.0052\n",
      "2019-03-08 15:44:41,742 - avg epoch 35 train loss: 0.0065\n",
      "2019-03-08 15:44:41,744 - **********TRAINING PERFORMANCE*********\n",
      "2019-03-08 15:45:44,853 - 0.7390329451404506\n",
      "2019-03-08 15:45:46,771 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.700     0.709     0.705      4913\n",
      "       I-LOC      0.463     0.820     0.592      1891\n",
      "      B-MISC      0.535     0.855     0.658      2173\n",
      "      I-MISC      0.430     0.785     0.556      3212\n",
      "       B-ORG      0.818     0.803     0.811      7390\n",
      "       I-ORG      0.754     0.791     0.772      4992\n",
      "       B-PER      0.722     0.831     0.773      4321\n",
      "       I-PER      0.774     0.905     0.834      3903\n",
      "\n",
      "   micro avg      0.666     0.806     0.729     32795\n",
      "   macro avg      0.649     0.813     0.713     32795\n",
      "weighted avg      0.695     0.806     0.739     32795\n",
      "\n",
      "2019-03-08 15:45:46,790 - **********VALIDATION PERFORMANCE*********\n",
      "2019-03-08 15:46:00,366 - 0.549441601506335\n",
      "2019-03-08 15:46:00,713 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.577     0.520     0.547      1084\n",
      "       I-LOC      0.246     0.588     0.347       325\n",
      "      B-MISC      0.251     0.528     0.341       339\n",
      "      I-MISC      0.250     0.494     0.332       557\n",
      "       B-ORG      0.684     0.634     0.658      1400\n",
      "       I-ORG      0.615     0.578     0.596      1104\n",
      "       B-PER      0.497     0.644     0.561       735\n",
      "       I-PER      0.531     0.760     0.625       634\n",
      "\n",
      "   micro avg      0.476     0.597     0.530      6178\n",
      "   macro avg      0.457     0.593     0.501      6178\n",
      "weighted avg      0.529     0.597     0.549      6178\n",
      "\n",
      "2019-03-08 15:46:00,722 - Loss: 0.6659618020057678\n",
      "2019-03-08 15:48:36,184 - avg epoch 36 train loss: 0.0070\n",
      "2019-03-08 15:51:18,146 - avg epoch 37 train loss: 0.0076\n",
      "2019-03-08 15:53:54,572 - avg epoch 38 train loss: 0.0071\n",
      "2019-03-08 15:56:31,509 - avg epoch 39 train loss: 0.0070\n",
      "2019-03-08 15:59:08,982 - avg epoch 40 train loss: 0.0063\n",
      "2019-03-08 15:59:08,983 - **********TRAINING PERFORMANCE*********\n",
      "2019-03-08 16:00:12,636 - 0.7468710807349751\n",
      "2019-03-08 16:00:14,572 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.693     0.715     0.704      4913\n",
      "       I-LOC      0.504     0.889     0.644      1891\n",
      "      B-MISC      0.548     0.847     0.666      2173\n",
      "      I-MISC      0.425     0.805     0.556      3212\n",
      "       B-ORG      0.838     0.788     0.812      7390\n",
      "       I-ORG      0.770     0.805     0.787      4992\n",
      "       B-PER      0.720     0.835     0.773      4321\n",
      "       I-PER      0.789     0.917     0.848      3903\n",
      "\n",
      "   micro avg      0.674     0.813     0.737     32795\n",
      "   macro avg      0.661     0.825     0.724     32795\n",
      "weighted avg      0.706     0.813     0.747     32795\n",
      "\n",
      "2019-03-08 16:00:14,592 - **********VALIDATION PERFORMANCE*********\n",
      "2019-03-08 16:00:27,643 - 0.5511663921593086\n",
      "2019-03-08 16:00:27,997 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.541     0.519     0.530      1084\n",
      "       I-LOC      0.253     0.606     0.357       325\n",
      "      B-MISC      0.266     0.519     0.352       339\n",
      "      I-MISC      0.255     0.510     0.340       557\n",
      "       B-ORG      0.700     0.604     0.648      1400\n",
      "       I-ORG      0.606     0.581     0.593      1104\n",
      "       B-PER      0.507     0.667     0.576       735\n",
      "       I-PER      0.574     0.787     0.664       634\n",
      "\n",
      "   micro avg      0.480     0.598     0.533      6178\n",
      "   macro avg      0.463     0.599     0.507      6178\n",
      "weighted avg      0.532     0.598     0.551      6178\n",
      "\n",
      "2019-03-08 16:00:28,002 - Loss: 0.6920409202575684\n",
      "2019-03-08 16:03:06,440 - avg epoch 41 train loss: 0.0060\n",
      "2019-03-08 16:05:45,048 - avg epoch 42 train loss: 0.0052\n",
      "2019-03-08 16:08:26,228 - avg epoch 43 train loss: 0.0047\n",
      "2019-03-08 16:11:06,606 - avg epoch 44 train loss: 0.0046\n",
      "2019-03-08 16:13:46,199 - avg epoch 45 train loss: 0.0044\n",
      "2019-03-08 16:13:46,201 - **********TRAINING PERFORMANCE*********\n",
      "2019-03-08 16:14:50,249 - 0.7578985739127038\n",
      "2019-03-08 16:14:52,186 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.739     0.708     0.723      4913\n",
      "       I-LOC      0.512     0.847     0.638      1891\n",
      "      B-MISC      0.570     0.846     0.681      2173\n",
      "      I-MISC      0.423     0.797     0.553      3212\n",
      "       B-ORG      0.858     0.812     0.834      7390\n",
      "       I-ORG      0.793     0.802     0.798      4992\n",
      "       B-PER      0.752     0.836     0.792      4321\n",
      "       I-PER      0.768     0.921     0.838      3903\n",
      "\n",
      "   micro avg      0.691     0.814     0.747     32795\n",
      "   macro avg      0.677     0.821     0.732     32795\n",
      "weighted avg      0.724     0.814     0.758     32795\n",
      "\n",
      "2019-03-08 16:14:52,207 - **********VALIDATION PERFORMANCE*********\n",
      "2019-03-08 16:15:04,271 - 0.5564012949117687\n",
      "2019-03-08 16:15:04,625 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.569     0.514     0.540      1084\n",
      "       I-LOC      0.269     0.545     0.360       325\n",
      "      B-MISC      0.278     0.504     0.358       339\n",
      "      I-MISC      0.245     0.458     0.319       557\n",
      "       B-ORG      0.741     0.620     0.675      1400\n",
      "       I-ORG      0.615     0.550     0.581      1104\n",
      "       B-PER      0.561     0.654     0.604       735\n",
      "       I-PER      0.530     0.806     0.640       634\n",
      "\n",
      "   micro avg      0.499     0.587     0.539      6178\n",
      "   macro avg      0.476     0.581     0.510      6178\n",
      "weighted avg      0.550     0.587     0.556      6178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-08 16:15:04,631 - Loss: 0.6858983635902405\n",
      "2019-03-08 16:17:44,831 - avg epoch 46 train loss: 0.0049\n",
      "2019-03-08 16:20:25,213 - avg epoch 47 train loss: 0.0052\n",
      "2019-03-08 16:23:06,634 - avg epoch 48 train loss: 0.0073\n",
      "2019-03-08 16:25:47,544 - avg epoch 49 train loss: 0.0072\n",
      "2019-03-08 16:28:28,366 - avg epoch 50 train loss: 0.0060\n",
      "2019-03-08 16:28:28,366 - **********TRAINING PERFORMANCE*********\n",
      "2019-03-08 16:29:46,090 - 0.7515934796303456\n",
      "2019-03-08 16:29:48,019 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.731     0.735     0.733      4913\n",
      "       I-LOC      0.501     0.864     0.634      1891\n",
      "      B-MISC      0.563     0.821     0.668      2173\n",
      "      I-MISC      0.451     0.788     0.574      3212\n",
      "       B-ORG      0.845     0.786     0.815      7390\n",
      "       I-ORG      0.766     0.799     0.782      4992\n",
      "       B-PER      0.717     0.850     0.778      4321\n",
      "       I-PER      0.768     0.919     0.837      3903\n",
      "\n",
      "   micro avg      0.685     0.812     0.743     32795\n",
      "   macro avg      0.668     0.820     0.728     32795\n",
      "weighted avg      0.713     0.812     0.752     32795\n",
      "\n",
      "2019-03-08 16:29:48,040 - **********VALIDATION PERFORMANCE*********\n",
      "2019-03-08 16:30:01,101 - 0.5439201419164589\n",
      "2019-03-08 16:30:01,454 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.533     0.510     0.521      1084\n",
      "       I-LOC      0.276     0.603     0.379       325\n",
      "      B-MISC      0.300     0.478     0.369       339\n",
      "      I-MISC      0.236     0.413     0.301       557\n",
      "       B-ORG      0.709     0.599     0.650      1400\n",
      "       I-ORG      0.609     0.548     0.577      1104\n",
      "       B-PER      0.516     0.646     0.574       735\n",
      "       I-PER      0.554     0.781     0.648       634\n",
      "\n",
      "   micro avg      0.490     0.575     0.530      6178\n",
      "   macro avg      0.467     0.572     0.502      6178\n",
      "weighted avg      0.534     0.575     0.544      6178\n",
      "\n",
      "2019-03-08 16:30:01,459 - Loss: 0.6981444954872131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5h 8min 56s, sys: 15min 10s, total: 5h 24min 7s\n",
      "Wall time: 2h 33min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print predictions before training\n",
    "#print_example(training_data, 123, model, token2idx, idx2tag)\n",
    "logger.info(\"START!\")\n",
    "train_loss, val_loss = [], []\n",
    "for epoch in range(EPOCHS): \n",
    "    train_loader = batches_generator(BATCH_SIZE, train_tokens, train_tags, seed=epoch)\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx_batch, batch in enumerate(train_loader):\n",
    "        batch_sents, batch_tags, batch_lens = batch\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Step 2. Run our forward pass.\n",
    "        tag_scores = model(batch_sents, batch_lens)\n",
    "        # Step 3. Compute the loss, gradients, and update the parameters\n",
    "        loss = model.loss(tag_scores, batch_tags)\n",
    "        loss.backward()\n",
    "        epoch_loss += float(loss)\n",
    "        clip_grad_norm_(model.parameters(), 5)\n",
    "        optimiser.step()\n",
    "        if (idx_batch + 1) % 970 == 0:\n",
    "            logger.info(f'Epoch [{epoch + 1}/{EPOCHS}], '\n",
    "                  f\"Step [{idx_batch + 1}/{len(train_tags)// BATCH_SIZE}], \"\n",
    "                  f\"Loss: {loss:.4f}\")\n",
    "        \n",
    "    logger.info(f\"avg epoch {epoch + 1} train loss: {epoch_loss/(idx_batch + 1):.4f}\")\n",
    "    if ((epoch + 1) % 5) == 0:\n",
    "        logger.info(\"**********TRAINING PERFORMANCE*********\")\n",
    "        loss = eval_model_for_set(model, train_tokens, train_tags)\n",
    "        train_loss.append(loss)\n",
    "        logger.info(\"**********VALIDATION PERFORMANCE*********\")\n",
    "        loss = eval_model_for_set(model, val_tokens, val_tags)\n",
    "        val_loss.append(loss)\n",
    "        logger.info(f\"Loss: {loss}\")\n",
    "\n",
    "# print predictions after training\n",
    "#print_example(training_data, 123, model, token2idx, idx2tag)\n",
    "#print(training_data[1][123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-08 13:33:40,768 - 0.8425425855009366\n",
      "2019-03-08 13:33:42,699 -               precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.880     0.848     0.864      4913\n",
      "       I-LOC      0.474     0.917     0.625      1891\n",
      "      B-MISC      0.655     0.947     0.774      2173\n",
      "      I-MISC      0.490     0.940     0.644      3212\n",
      "       B-ORG      0.933     0.923     0.928      7390\n",
      "       I-ORG      0.849     0.935     0.890      4992\n",
      "       B-PER      0.817     0.902     0.858      4321\n",
      "       I-PER      0.801     0.984     0.883      3903\n",
      "\n",
      "   micro avg      0.754     0.921     0.829     32795\n",
      "   macro avg      0.737     0.924     0.808     32795\n",
      "weighted avg      0.793     0.921     0.843     32795\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4153)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model_for_set(model, train_tokens, train_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From hereafter, things to experiment, e.g. Dropout"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python (marc)",
   "language": "python",
   "name": "marc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
