{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $(token, pos, tag)^N$ --> $(tokens, tags)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_tags_from_sents(sents):\n",
    "    tokens, tags = [], []\n",
    "    for sent in sents:\n",
    "        sent_tokens, _, sent_tags = list(zip(*sent))\n",
    "        tokens.append(sent_tokens)\n",
    "        tags.append(sent_tags)\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.9 ms, sys: 6.83 ms, total: 70.7 ms\n",
      "Wall time: 70.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_tokens, train_tags = get_tokens_tags_from_sents(train_sents)\n",
    "val_tokens, val_tags = get_tokens_tags_from_sents(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always understand what kind of data you deal with. For this purpose, you can print the data running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>(</td>\n",
       "      <td>Australia</td>\n",
       "      <td>)</td>\n",
       "      <td>,</td>\n",
       "      <td>25</td>\n",
       "      <td>may</td>\n",
       "      <td>(</td>\n",
       "      <td>EFE</td>\n",
       "      <td>)</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>B-LOC</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  1          2  3  4   5    6  7      8  9  10\n",
       "0  Melbourne  (  Australia  )  ,  25  may  (    EFE  )  .\n",
       "1      B-LOC  O      B-LOC  O  O   O    O  O  B-ORG  O  O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 0\n",
    "pd.DataFrame([train_tokens[idx], train_tags[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare mappings\n",
    "\n",
    "To train a neural network, we will use two mappings: \n",
    "- {token}$\\to${token id}: address the row in embeddings matrix for the current token;\n",
    "- {tag}$\\to${tag id}: one-hot ground truth probability distribution vectors for computing the loss at the output of the network.\n",
    "\n",
    "Now you need to implement the function *build_dict* which will return {token or tag}$\\to${index} and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(tokens_or_tags, special_tokens):\n",
    "    # Create a dictionary with default value 0\n",
    "    tok2idx = defaultdict(lambda: 0)\n",
    "    ind = 0\n",
    "    for t in special_tokens:\n",
    "        tok2idx[t] = ind\n",
    "        ind += 1\n",
    "    for sam in tokens_or_tags:\n",
    "        for t in sam:\n",
    "            if t not in special_tokens and t not in tok2idx:\n",
    "                tok2idx[t] = ind\n",
    "                ind += 1\n",
    "    return tok2idx, dict((v, k) for k, v in tok2idx.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the function *build_dict* you can make dictionaries for tokens and tags. Special tokens for tokens will be:\n",
    " - `<UNK>` token for out of vocabulary tokens; index = 0\n",
    " - `<PAD>` token for padding sentence to the same length when we create batches of sentences. index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['O']\n",
    "\n",
    "# Create dictionaries \n",
    "token2idx, idx2token = build_dict(train_tokens, special_tokens)\n",
    "tag2idx, idx2tag = build_dict(train_tags, special_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate batches\n",
    "\n",
    "Neural Networks are usually trained with batches. It means that weight updates of the network are based on several sequences at every single time. The tricky part is that all sequences within a batch need to have the same length. So we will pad them with a special `<PAD>` token. It is also a good practice to provide RNN with sequence lengths, so it can skip computations for padding parts. We provide the batching function *batches_generator* readily available for you to save time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    def to_ix_defaults(t):\n",
    "        return to_ix.get(t, to_ix.get(\"<UNK>\", to_ix['O']))\n",
    "    return torch.tensor(np.vectorize(to_ix_defaults)(seq), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(batch_size, tokens, tags,\n",
    "                      shuffle=True, allow_smaller_last_batch=True, seed=8):\n",
    "    \"\"\"Generates padded batches of tokens and tags.\"\"\"\n",
    "    # TODO: use DataLoader from Pytorch for this\n",
    "    # tokens is a list of docs, and each docs is a list of tokens\n",
    "    # SHUFFLE\n",
    "    n_samples = len(tokens)\n",
    "    np.random.seed(seed)\n",
    "    if shuffle:\n",
    "        order = np.random.permutation(n_samples)\n",
    "    else:\n",
    "        order = np.arange(n_samples)\n",
    "\n",
    "    # NUMBER OF BATCHES\n",
    "    n_batches = n_samples // batch_size\n",
    "    # and n_samples / batch_size not integer, put the leftovers in last batch\n",
    "    if allow_smaller_last_batch and n_samples % batch_size:\n",
    "        n_batches += 1\n",
    "\n",
    "    # for each batch, get the docs, labels and real lengths and yield them\n",
    "    for k in range(n_batches):\n",
    "        batch_start = k * batch_size\n",
    "        batch_end = min((k + 1) * batch_size, n_samples)\n",
    "        x, y = [], []\n",
    "        batch_lengths = torch.zeros(batch_end - batch_start, dtype=torch.int32)\n",
    "        # x will be a list of lists of indices (one list of indices per doc in this batch)\n",
    "        for sample_in_batch_index, sample_idx in enumerate(order[batch_start: batch_end]):\n",
    "            try:\n",
    "                x.append(prepare_sequence(tokens[sample_idx], token2idx))\n",
    "                y.append(prepare_sequence(tags[sample_idx], tag2idx))\n",
    "            except Exception as marc:\n",
    "                print(marc)\n",
    "                import pdb; pdb.set_trace()\n",
    "            batch_lengths[sample_in_batch_index] = len(tags[sample_idx])\n",
    "        x = pad_sequence(x, batch_first=True, padding_value=token2idx[\"<PAD>\"])\n",
    "        y = pad_sequence(y, batch_first=True, padding_value=-1)\n",
    "        batch_lengths, perm_idx = batch_lengths.sort(0, descending=True)\n",
    "        # yield each batch\n",
    "        yield x[perm_idx, ...], y[perm_idx, ...], batch_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.nn.utils.rnn import (\n",
    "    pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
    ")\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "torch.manual_seed(1)\n",
    "import numpy as np\n",
    "from sklearn_crfsuite.metrics import flat_classification_report, flat_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, embedding_dim, hidden_dim, vocab_size, tagset_size,\n",
    "                padding_idx, verbose=False, bidirectional=False):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,\n",
    "                           bidirectional=bidirectional)\n",
    "        self.tagset_size = tagset_size\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear((1+bidirectional)*hidden_dim, tagset_size)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def forward(self, X, X_lens):\n",
    "        # embeddings\n",
    "        embeds = self.word_embeddings(X)\n",
    "        if self.verbose: print(f\"Embeds: {embeds.size()}\")\n",
    "        # pack_padded_sequence so that padded items in the sequence won't be shown to the LSTM\n",
    "        embeds = pack_padded_sequence(embeds, X_lens.cpu().numpy(), batch_first=True)\n",
    "        # lstm\n",
    "        #lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        # undo the packing operation\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        if self.verbose: print(f\"lstm_out: {lstm_out.size()}\")\n",
    "        # (batch_size, seq_len, hidden_dim) --> (batch_size * seq_len, hidden_dim)\n",
    "        s = lstm_out.contiguous().view(-1, lstm_out.shape[2])\n",
    "        # (batch_size * seq_len, hidden_dim) --> (batch_size * seq_len, tag_dim)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        if self.verbose: print(f\"tag space: {tag_space.size()}\")\n",
    "        # normalize logits\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        if self.verbose: print(f\"tag scores: {tag_scores.size()}\")\n",
    "        return tag_scores\n",
    "    \n",
    "    def loss(self, Y_hat, Y):\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        return criterion(Y_hat.view(-1, Y_hat.size()[2]), Y.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-ORG', 'B-PER', 'I-PER', 'B-MISC', 'I-ORG', 'I-LOC', 'I-MISC']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_score = list(tag2idx.keys())\n",
    "labels_to_score.remove('O')\n",
    "labels_to_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels_to_score, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, batch_tokens, batch_lengths, batch_tags=None):\n",
    "    \"\"\"Performs predictions and transforms indices to tokens and tags.\"\"\"\n",
    "    \n",
    "    tag_scores = model(batch_tokens, batch_lengths)\n",
    "    predicted_tags = np.vectorize(idx2tag.get)(torch.argmax(tag_scores, dim=2).data.numpy())\n",
    "    if batch_tags is not None:\n",
    "        return predicted_tags, model.loss(tag_scores, batch_tags)\n",
    "    return predicted_tags\n",
    "\n",
    "\n",
    "def my_scorer(true_tags, predicted_tags):\n",
    "    score = flat_f1_score(true_tags, predicted_tags, average='weighted', labels=sorted_labels)\n",
    "    logger.info(f\"f1 score: {score:.3f}\")\n",
    "    print(flat_classification_report(\n",
    "        true_tags, predicted_tags, labels=sorted_labels, digits=3\n",
    "    ))\n",
    "\n",
    "\n",
    "def eval_model_for_set(model, tokens, tags, scoring=my_scorer):\n",
    "    \"\"\"Computes NER quality measures given model and a dataset\"\"\"\n",
    "    model.eval()\n",
    "    predicted_tags, true_tags, loss = [], [], 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch, lengths in batches_generator(len(tokens), tokens, tags):\n",
    "            padded_predicted_tags, batch_loss = predict_tags(model, x_batch, lengths, y_batch)\n",
    "            loss += batch_loss\n",
    "            padded_true_tags = np.vectorize(idx2tag.get)(y_batch.data)\n",
    "            for x, y, l in zip(padded_predicted_tags, padded_true_tags, lengths): \n",
    "                predicted_tags.append(x[:l])\n",
    "                true_tags.append(y[:l])    \n",
    "        scoring(true_tags, predicted_tags)\n",
    "        return loss / len(true_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparams and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 200\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "VOCAB_SIZE = len(token2idx)\n",
    "TAGSET_SIZE = len(tag2idx)\n",
    "PADDING_IDX = token2idx[\"<PAD>\"]\n",
    "training_data = (train_tokens, train_tags)\n",
    "model = LSTMTagger(BATCH_SIZE, EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, \n",
    "                   TAGSET_SIZE, PADDING_IDX, verbose=False, bidirectional=True)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print predictions before training\n",
    "#print_example(training_data, 123, model, token2idx, idx2tag)\n",
    "logger.info(\"START!\")\n",
    "train_loss, val_loss = [], []\n",
    "for epoch in range(EPOCHS): \n",
    "    train_loader = batches_generator(BATCH_SIZE, train_tokens, train_tags, seed=epoch)\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for idx_batch, batch in enumerate(train_loader):\n",
    "        batch_sents, batch_tags, batch_lens = batch\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "        # Step 2. Run our forward pass.\n",
    "        tag_scores = model(batch_sents, batch_lens)\n",
    "        # Step 3. Compute the loss, gradients, and update the parameters\n",
    "        loss = model.loss(tag_scores, batch_tags)\n",
    "        loss.backward()\n",
    "        epoch_loss += float(loss)\n",
    "        clip_grad_norm_(model.parameters(), 5)\n",
    "        optimiser.step()\n",
    "        # disabled for now\n",
    "        if (idx_batch + 1) % 970 == 0:\n",
    "            logger.info(\n",
    "                f'Epoch [{epoch + 1}/{EPOCHS}], '\n",
    "                f\"Step [{idx_batch + 1}/{len(train_tags)// BATCH_SIZE}], \"\n",
    "                f\"Loss: {loss:.4f}\"\n",
    "            )\n",
    "        \n",
    "    logger.info(f\"avg epoch {epoch + 1} train loss: {epoch_loss/(idx_batch + 1):.4f}\")\n",
    "    if ((epoch + 1) % 5) == 0:\n",
    "        logger.info(\"**********TRAINING PERFORMANCE*********\")\n",
    "        train_loss.append(eval_model_for_set(model, train_tokens, train_tags))\n",
    "        logger.info(f\"Loss: {train_loss[-1]}\")\n",
    "        logger.info(\"**********VALIDATION PERFORMANCE*********\")\n",
    "        val_loss.append(eval_model_for_set(model, val_tokens, val_tags))\n",
    "        logger.info(f\"Loss: {val_loss[-1]}\")\n",
    "\n",
    "# print predictions after training\n",
    "#print_example(training_data, 123, model, token2idx, idx2tag)\n",
    "#print(training_data[1][123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas to improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Accuracy:\n",
    "* Dropout\n",
    "* Early stopping\n",
    "* Fine-tunning hyperparams: learning rate (https://www.jeremyjordan.me/nn-learning-rate/), embedding and hidden dimensions\n",
    "* Use trained embeddings\n",
    "* CRF / CNN\n",
    "\n",
    "Coding:\n",
    "* Use `DataLoader` from Pytorch rather than `batches_generator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(training_data, i, model, word2idx, idx2tag):\n",
    "    pass\n",
    "    # Note that element i,j of tag_scores is the score for tag j for word i.\n",
    "    # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "    # with torch.no_grad():\n",
    "    #     seq = training_data[0][i]\n",
    "    #     labs = training_data[1][i]\n",
    "    #     inputs = prepare_sequence(seq, word2idx)\n",
    "    #     tag_scores = model(inputs.view(1, len(inputs)),\n",
    "    #                        torch.tensor([len(seq)]))\n",
    "    #     tags = np.vectorize(idx2tag.get)(torch.argmax(tag_scores, dim=2).data.numpy())\n",
    "    #     print(seq)\n",
    "    #     print()\n",
    "    #     print(tags)\n",
    "    #     print()\n",
    "    #     print(len(seq), tag_scores.size(), tags.shape)\n",
    "    #     print()\n",
    "    #     print(training_data[1][i])\n",
    "    #     print(training_data[1][i] == tags)\n",
    "#print_example(training_data, 79, model, token2idx, idx2tag)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python (marc)",
   "language": "python",
   "name": "marc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
